{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sys import stderr\n",
    "from imageio import imread, imwrite\n",
    "import numpy as np\n",
    "from skimage import transform as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from transform_utils import scale_pixel_box_coordinates, crop_image\n",
    "\n",
    "SAVING_PLOT = False\n",
    "JUPYTER = True\n",
    "\n",
    "osx_dir = Path(\"osx/catalina/\").absolute()\n",
    "source_dir = osx_dir / \"png\"\n",
    "preproc_dir = osx_dir / \"bg/\"\n",
    "png_dir = Path(\"enlarged/\").absolute()\n",
    "out_dir = Path(\"transparent/\").absolute()\n",
    "png = png_dir / \"glyph-u1F343.png\"\n",
    "osx_bw_db = osx_dir / \"emoji_bw_calc.db\"\n",
    "NO_OVERWRITE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compare_one_sr_alpha_mask import get_emoji_rgb_bg, plot_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09301cff1d3c4afbb85eeba9a537b67e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    }
   ],
   "source": [
    "source_png = source_dir / png.name\n",
    "preproc_png = preproc_dir / png.name\n",
    "output_png = out_dir / png.name\n",
    "if output_png.exists() and NO_OVERWRITE:\n",
    "    raise ValueError(\"Cannot overwrite\")\n",
    "elif not source_png.exists():\n",
    "    raise NameError(f\"Expected '{source_png}' corresponding to input '{png.name}'\")\n",
    "# Store (x,y) coordinates of the box of interest\n",
    "box_top_l = (0,104)\n",
    "box_bot_r = (56,160)\n",
    "box = [box_top_l, box_bot_r]\n",
    "# Remove the mask and show the result\n",
    "img = imread(png)\n",
    "source_img = imread(source_png)\n",
    "preproc_img = imread(preproc_png)\n",
    "scale = img.shape[0] / source_img.shape[0]\n",
    "scaled_box = scale_pixel_box_coordinates(box, scale)\n",
    "source_img_sub = crop_image(source_img, box)\n",
    "preproc_img_sub = crop_image(preproc_img, box)\n",
    "source_img_sub_alpha = source_img_sub[:,:,3]\n",
    "img_sub = crop_image(img, scaled_box)\n",
    "scaled_preproc_img_sub = tf.resize(\n",
    "    preproc_img_sub[:,:,:3], img_sub.shape, order=0\n",
    ")\n",
    "scaled_source_img_sub = tf.resize(\n",
    "    source_img_sub[:,:,:3], img_sub.shape, order=0\n",
    ")\n",
    "scaled_source_img_sub_alpha = tf.resize(\n",
    "    source_img_sub_alpha, img_sub[:,:,0].shape, order=0\n",
    ")\n",
    "scaled_preproc_img_sub *= (1/scaled_preproc_img_sub.max()) * 255\n",
    "scaled_preproc_img_sub = scaled_preproc_img_sub.astype(int)#img.dtype)\n",
    "scaled_source_img_sub *= (1/scaled_source_img_sub.max()) * 255\n",
    "scaled_source_img_sub = scaled_source_img_sub.astype(int)#img.dtype)\n",
    "scaled_source_img_sub_alpha *= (1/scaled_source_img_sub_alpha.max()) * 255\n",
    "scaled_source_img_sub_alpha = scaled_source_img_sub_alpha.astype(int)#img.dtype)\n",
    "scaled_source_img_sub_im = scaled_source_img_sub.copy() # Retain 3 channel copy\n",
    "scaled_source_img_sub = np.insert(scaled_source_img_sub, 3, scaled_source_img_sub_alpha, axis=2)\n",
    "composited_grad = img_sub.astype(int) - scaled_preproc_img_sub\n",
    "# Rescale from [-255,+255] to [0,1] by incrementing +255 then squashing by half\n",
    "composited_grad = ((composited_grad + 255) / (255*2))\n",
    "composited_grad *= scaled_source_img_sub_alpha[:,:,None]\n",
    "composited_grad /= 255\n",
    "# Rescale all opaque regions to 1 (and clip any above 1 now)\n",
    "previous_max_alpha = scaled_source_img_sub_alpha == 255\n",
    "min_of_previous_max_alpha = composited_grad[previous_max_alpha].min()\n",
    "composited_grad *= (0.5/min_of_previous_max_alpha)\n",
    "#composited_grad[scaled_source_img_sub_alpha == 255] = 0.5\n",
    "#composited_grad /= composited_grad.max()\n",
    "#breakpoint()\n",
    "decomp_alpha = (scaled_source_img_sub_alpha / 255) + composited_grad.max(axis=2)\n",
    "# Now rearrange to acquire the estimatable part (the \"estimand\")\n",
    "i_in = (scaled_source_img_sub_alpha[:,:,None]/255) * (scaled_source_img_sub_im/255) # alpha_source * im_source\n",
    "#breakpoint()\n",
    "# If squashing composited_grad to [0,1] then don't need to divide it by 255 here\n",
    "#estimand = (composited_grad/255) - (scaled_source_img_sub_alpha[:,:,None] * i_in)\n",
    "estimand = composited_grad - (scaled_source_img_sub_alpha[:,:,None] * i_in)\n",
    "fig1, f1_axes = plot_comparison(\n",
    "    scaled_source_img_sub_alpha,\n",
    "    scaled_source_img_sub,\n",
    "    img_sub,\n",
    "    composited_grad,\n",
    "    decomp_alpha,\n",
    "    SAVING_PLOT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an estimate of the LR-SR alpha channel difference, we can add that to the LR alpha to get an estimate of SR alpha, then decomposite the LIIF SR RGB image onto the (known) background to regain [an estimate for] the SR RGBA image (whose quality we can check by changing the background colour via a call to `matplotlib.pyplot.imshow`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposited = np.insert(img_sub, 3, decomp_alpha * 255, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1710f3a300f640a1954946cdcc0800b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f91e9e2dfa0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_black = np.zeros_like(scaled_source_img_sub, dtype=img.dtype)\n",
    "all_black[:,:,3] = 255\n",
    "fig2, f2_axes = plot_comparison(\n",
    "    scaled_source_img_sub_alpha,\n",
    "    scaled_source_img_sub,\n",
    "    img_sub,\n",
    "    composited_grad,\n",
    "    all_black,\n",
    "    SAVING_PLOT\n",
    ")\n",
    "f2_axes[-1].imshow(decomposited)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emoji_liif",
   "language": "python",
   "name": "emoji_liif"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
